{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bb515e-2b1f-4d6b-a93a-508c9ca0f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you downloaded and saved as top-1k.csv\n",
    "df = pd.read_csv(\"tranco.csv\", header=None)\n",
    "df.columns = ['rank', 'domain']\n",
    "\n",
    "# Add protocol and create URL\n",
    "df['url'] = 'https://' + df['domain']\n",
    "df['label'] = 0  # label = 0 means legitimate\n",
    "\n",
    "df[['url', 'label']].to_csv(\"legit_urls.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cfd0ade-be20-457c-9f36-64e4e413c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# OpenPhish free feed (you can visit it to see how the data looks)\n",
    "URL = \"https://openphish.com/feed.txt\"\n",
    "\n",
    "response = requests.get(URL)\n",
    "phishing_urls = response.text.strip().split('\\n')\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"phishing_urls.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"url\", \"label\"])  # label = 1 means phishing\n",
    "    for url in phishing_urls:\n",
    "        writer.writerow([url, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9874a77b-f329-4430-8cfe-da69fd4e7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# OpenPhish free feed (you can visit it to see how the data looks)\n",
    "URL = \"https://urlhaus.abuse.ch/downloads/text/\"\n",
    "\n",
    "response = requests.get(URL)\n",
    "phishing_urls = response.text.strip().split('\\n')\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"phishing_urls2.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"url\", \"label\"])  # label = 1 means phishing\n",
    "    for url in phishing_urls:\n",
    "        writer.writerow([url, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "751c51b0-7dc6-4839-80e1-a1f4987045d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to urls.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load both CSV files (with label column already present)\n",
    "df_legit = pd.read_csv('legit_urls.csv')\n",
    "df_phish = pd.read_csv('phishing_urls.csv')\n",
    "\n",
    "# Concatenate them\n",
    "df_combined = pd.concat([df_legit, df_phish], ignore_index=True)\n",
    "\n",
    "# Optional: Shuffle the combined dataset\n",
    "df_combined = df_combined.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save to a new CSV file\n",
    "df_combined.to_csv('urls.csv', index=False)\n",
    "\n",
    "print(\"Combined dataset saved to urls.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e85181f-f18e-4b90-b6ec-16c6223103ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    1000000\n",
      "1.0     110854\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the combined dataset\n",
    "df = pd.read_csv('urls.csv')\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98d56b1f-4ce8-49f7-a441-4e1b3a2a35eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates. 1110855 unique URLs saved to urls_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(\"urls.csv\")\n",
    "\n",
    "# Drop duplicate URLs based on the 'url' column\n",
    "df.drop_duplicates(subset='url', inplace=True)\n",
    "\n",
    "# Save the cleaned DataFrame back to CSV\n",
    "df.to_csv(\"urls.csv\", index=False)\n",
    "\n",
    "print(f\"Removed duplicates. {len(df)} unique URLs saved to urls_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b97f273-b854-40a7-b3e3-cdc1135cc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tldextract\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Load the CSV with URLs\n",
    "df = pd.read_csv(\"phishing_dataset_features.csv\")  # Combine phishing + legit CSVs\n",
    "\n",
    "# Feature extraction functions\n",
    "def has_ip(url):\n",
    "    return 1 if re.match(r\"http[s]?://\\d{1,3}(\\.\\d{1,3}){3}\", url) else 0\n",
    "\n",
    "def url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def count_dots(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def has_at_symbol(url):\n",
    "    return 1 if '@' in url else 0\n",
    "\n",
    "def uses_https(url):\n",
    "    return 1 if url.startswith(\"https\") else 0\n",
    "\n",
    "def get_domain(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return ext.domain\n",
    "# Helper: Entropy calculator\n",
    "def shannon_entropy(string):\n",
    "    prob = [float(string.count(c)) / len(string) for c in set(string)]\n",
    "    return -sum([p * math.log(p) / math.log(2.0) for p in prob])\n",
    "\n",
    "# Shorteners to check\n",
    "shorteners = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co', 'is.gd', 'buff.ly']\n",
    "\n",
    "def count_hyphens(url):\n",
    "    return url.count('-')\n",
    "\n",
    "def count_subdomains(url):\n",
    "    hostname = urlparse(url).hostname or ''\n",
    "    return hostname.count('.') - 1\n",
    "\n",
    "def has_suspicious_words(url):\n",
    "    words = ['login', 'verify', 'secure', 'update', 'account', 'banking']\n",
    "    return any(word in url.lower() for word in words)\n",
    "\n",
    "def get_url_entropy(url):\n",
    "    return shannon_entropy(url)\n",
    "\n",
    "def is_short_url(url):\n",
    "    return any(short in url for short in shorteners)\n",
    "\n",
    "def get_path_len(url):\n",
    "    return len(urlparse(url).path)\n",
    "\n",
    "def get_query_len(url):\n",
    "    return len(urlparse(url).query)\n",
    "\n",
    "def count_special_chars(url):\n",
    "    return sum(url.count(c) for c in ['@', '&', '%', '=', '?', '_'])\n",
    "\n",
    "def has_http_token(url):\n",
    "    path = urlparse(url).path\n",
    "    return 1 if 'http' in path else 0\n",
    "\n",
    "\n",
    "def has_port(url):\n",
    "    return 1 if urlparse(url).port else 0\n",
    "\n",
    "def domain_length(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return len(ext.domain)\n",
    "\n",
    "suspicious_tlds = ['tk', 'ga', 'ml', 'cf', 'gq']\n",
    "def has_suspicious_tld(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return 1 if ext.suffix in suspicious_tlds else 0\n",
    "\n",
    "def https_in_domain(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    full_domain = f\"{ext.subdomain}.{ext.domain}.{ext.suffix}\"\n",
    "    return 1 if 'https' in full_domain else 0\n",
    "\n",
    "def hostname_length(url):\n",
    "    hostname = urlparse(url).hostname or ''\n",
    "    return len(hostname)\n",
    "def count_digits(url):\n",
    "    return sum(c.isdigit() for c in url)\n",
    "\n",
    "\n",
    "# Extract features\n",
    "df['url_len'] = df['url'].apply(url_length)\n",
    "df['num_dots'] = df['url'].apply(count_dots)\n",
    "df['has_ip'] = df['url'].apply(has_ip)\n",
    "df['has_at'] = df['url'].apply(has_at_symbol)\n",
    "df['uses_https'] = df['url'].apply(uses_https)\n",
    "df['domain'] = df['url'].apply(get_domain)\n",
    "df['num_hyphens'] = df['url'].apply(count_hyphens)\n",
    "df['num_subdomains'] = df['url'].apply(count_subdomains)\n",
    "df['has_suspicious_words'] = df['url'].apply(has_suspicious_words)\n",
    "df['url_entropy'] = df['url'].apply(get_url_entropy)\n",
    "df['is_shortened'] = df['url'].apply(is_short_url)\n",
    "df['path_length'] = df['url'].apply(get_path_len)\n",
    "df['query_length'] = df['url'].apply(get_query_len)\n",
    "df['num_special_chars'] = df['url'].apply(count_special_chars)\n",
    "df['has_http_token'] = df['url'].apply(has_http_token)\n",
    "df['has_port'] = df['url'].apply(has_port)\n",
    "df['domain_len'] = df['url'].apply(domain_length)\n",
    "df['suspicious_tld'] = df['url'].apply(has_suspicious_tld)\n",
    "df['https_in_domain'] = df['url'].apply(https_in_domain)\n",
    "df['hostname_len'] = df['url'].apply(hostname_length)\n",
    "df['digit_count'] = df['url'].apply(count_digits)\n",
    "\n",
    "\n",
    "# Save feature-rich dataset\n",
    "df.to_csv(\"phishing_dataset_features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f0c39-fd74-4264-a0f3-c6292a3e1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"url.csv\")\n",
    "\n",
    "# Drop rows where 'url' column is missing or empty\n",
    "df = df[df['url'].notna()]             # Removes NaN values\n",
    "df = df[df['url'].str.strip() != \"\"]   # Removes empty strings or spaces\n",
    "\n",
    "# Optionally, reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_csv(\"phishing_dataset_features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35333c5e-f8be-429c-b91c-5eeb057e949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    1000000\n",
      "1.0     110854\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "df = pd.read_csv('phishing_dataset_features.csv')\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b033cf9-609c-4074-ae0c-282e9b66486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "label\n",
      "0.0    1000000\n",
      "1.0     110854\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced class distribution:\n",
      "label\n",
      "1.0    110854\n",
      "0.0    110854\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"phishing_dataset_features.csv\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Original class distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Separate classes\n",
    "malicious = df[df['label'] == 1.0]\n",
    "benign = df[df['label'] == 0.0]\n",
    "\n",
    "# Downsample benign to match malicious count\n",
    "benign_sampled = benign.sample(n=len(malicious), random_state=42)\n",
    "\n",
    "# Combine balanced dataset\n",
    "balanced_df = pd.concat([malicious, benign_sampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save to new CSV\n",
    "balanced_df.to_csv(\"phishing_dataset_balanced.csv\", index=False)\n",
    "\n",
    "print(\"\\nBalanced class distribution:\")\n",
    "print(balanced_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1987906d-5075-4704-b4c5-38c161589116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    5000\n",
      "1.0    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set sample size per class\n",
    "sample_size = 5000  # or any number you prefer\n",
    "\n",
    "# Split by label\n",
    "phishing_df = df[df['label'] == 1.0].sample(n=sample_size, random_state=42)\n",
    "legit_df = df[df['label'] == 0.0].sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_df = pd.concat([phishing_df, legit_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Now balanced_df has 5000 phishing + 5000 legitimate = 10,000 total\n",
    "print(balanced_df['label'].value_counts())\n",
    "balanced_df.to_csv(\"balanced_phishing_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc718a2-de88-480b-9cbe-5942e43228f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tldextract\n",
    "import requests\n",
    "import whois\n",
    "import socket\n",
    "import ipinfo\n",
    "import logging\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache\n",
    "\n",
    "# Suppress whois logger\n",
    "logging.getLogger('whois').setLevel(logging.CRITICAL)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"balanced_phishing_dataset.csv\")\n",
    "\n",
    "# Initialize IPinfo handler\n",
    "access_token = '680060c702415c'  # Replace with your own token\n",
    "ip_handler = ipinfo.getHandler(access_token)\n",
    "\n",
    "# ---------------------- WHOIS DOMAIN AGE ---------------------- #\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_domain_age_cached(domain):\n",
    "    try:\n",
    "        socket.gethostbyname(domain)\n",
    "        info = whois.whois(domain)\n",
    "        creation = info.creation_date\n",
    "        if isinstance(creation, list):\n",
    "            creation = min(creation)\n",
    "        if creation:\n",
    "            if isinstance(creation, str):\n",
    "                creation = datetime.strptime(creation, \"%Y-%m-%d %H:%M:%S\")\n",
    "            return (datetime.now() - creation).days\n",
    "    except Exception:\n",
    "        return -1\n",
    "    return -1\n",
    "\n",
    "def extract_domain(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return f\"{ext.domain}.{ext.suffix}\" if ext.suffix else \"\"\n",
    "\n",
    "def get_domain_age_from_url(url):\n",
    "    domain = extract_domain(url)\n",
    "    return get_domain_age_cached(domain)\n",
    "\n",
    "# ---------------------- HTTP HEADER INFO ---------------------- #\n",
    "\n",
    "def get_headers_info(url):\n",
    "    try:\n",
    "        response = requests.head(\n",
    "            url,\n",
    "            timeout=5,\n",
    "            allow_redirects=True,\n",
    "            headers={'User-Agent': 'Mozilla/5.0'}\n",
    "        )\n",
    "        return {\n",
    "            \"header_status_code\": response.status_code,\n",
    "            \"header_server\": response.headers.get(\"Server\", \"\"),\n",
    "            \"header_powered_by\": response.headers.get(\"X-Powered-By\", \"\"),\n",
    "            \"header_has_csp\": \"Content-Security-Policy\" in response.headers\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"header_status_code\": -1,\n",
    "            \"header_server\": \"\",\n",
    "            \"header_powered_by\": \"\",\n",
    "            \"header_has_csp\": False\n",
    "        }\n",
    "\n",
    "# ---------------------- IP GEOLOCATION ---------------------- #\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_geo_info_cached(hostname):\n",
    "    try:\n",
    "        ip = socket.gethostbyname(hostname)\n",
    "        details = ip_handler.getDetails(ip)\n",
    "        return {\n",
    "            \"geo_country\": details.country or \"NA\",\n",
    "            \"geo_org\": details.org or \"NA\",\n",
    "            \"geo_asn\": details.all.get(\"asn\", {}).get(\"asn\", \"NA\")\n",
    "        }\n",
    "    except:\n",
    "        return {\"geo_country\": \"NA\", \"geo_org\": \"NA\", \"geo_asn\": \"NA\"}\n",
    "\n",
    "def get_ip_geo_from_url(url):\n",
    "    try:\n",
    "        hostname = urlparse(url).hostname\n",
    "        if not hostname or not re.match(r\"^((?!-)[A-Za-z0-9-]{1,63}(?<!-)\\.)+[A-Za-z]{2,6}$\", hostname):\n",
    "            return {\"geo_country\": \"NA\", \"geo_org\": \"NA\", \"geo_asn\": \"NA\"}\n",
    "        return get_geo_info_cached(hostname)\n",
    "    except:\n",
    "        return {\"geo_country\": \"NA\", \"geo_org\": \"NA\", \"geo_asn\": \"NA\"}\n",
    "\n",
    "# ---------------------- APPLY IN PARALLEL ---------------------- #\n",
    "\n",
    "# Apply domain age in parallel\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    df['domain_age_days'] = list(executor.map(get_domain_age_from_url, df['url']))\n",
    "\n",
    "# Apply header info in parallel\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    headers = list(executor.map(get_headers_info, df['url']))\n",
    "df = df.join(pd.json_normalize(headers))\n",
    "\n",
    "# Apply geo info in parallel\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    geo_info = list(executor.map(get_ip_geo_from_url, df['url']))\n",
    "df = df.join(pd.json_normalize(geo_info))\n",
    "\n",
    "# ---------------------- SAVE OUTPUT ---------------------- #\n",
    "\n",
    "df.to_csv(\"phishing_dataset_enriched.csv\", index=False)\n",
    "print(\"âœ… Enrichment complete and saved to 'phishing_dataset_enriched.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bee4c-fb10-4d5c-8d5b-657a6f330f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
